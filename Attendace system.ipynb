{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition Based Attendance System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn import neighbors\n",
    "from os import listdir\n",
    "from os.path import isdir, join, isfile, splitext\n",
    "import pickle\n",
    "from PIL import Image, ImageFont, ImageDraw, ImageEnhance\n",
    "import face_recognition\n",
    "from face_recognition import face_locations\n",
    "from face_recognition.cli import image_files_in_folder\n",
    "\n",
    "ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg'}\n",
    "\n",
    "def train(train_dir, model_save_path = \"knn_model.sav\", n_neighbors = None, knn_algo = 'ball_tree', verbose=False):\n",
    "    \"\"\"\n",
    "    Trains a k-nearest neighbors classifier for face recognition.\n",
    "    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n",
    "     (View in source code to see train_dir example tree structure)\n",
    "     Structure:\n",
    "        <train_dir>/\n",
    "        ├── <person1>/\n",
    "        │   ├── <somename1>.jpeg\n",
    "        │   ├── <somename2>.jpeg\n",
    "        │   ├── ...\n",
    "        ├── <person2>/\n",
    "        │   ├── <somename1>.jpeg\n",
    "        │   └── <somename2>.jpeg\n",
    "        └── ...\n",
    "    :param model_save_path: (optional) path to save model of disk\n",
    "    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified.\n",
    "    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n",
    "    :param verbose: verbosity of training\n",
    "    :return: returns knn classifier that was trained on the given data.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for class_dir in listdir(train_dir):\n",
    "        if not isdir(join(train_dir, class_dir)):\n",
    "            continue\n",
    "        for img_path in image_files_in_folder(join(train_dir, class_dir)):\n",
    "            image = face_recognition.load_image_file(img_path)\n",
    "            faces_bboxes = face_locations(image)\n",
    "            print(faces_bboxes)\n",
    "            if len(faces_bboxes) != 1:\n",
    "                if verbose:\n",
    "                    print(\"image {} not fit for training: {}\".format(img_path, \"didn't find a face\" if len(faces_bboxes) < 1 else \"found more than one face\"))\n",
    "                continue\n",
    "            X.append(face_recognition.face_encodings(image, known_face_locations=faces_bboxes)[0])\n",
    "            y.append(class_dir)\n",
    "\n",
    "\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int(round(sqrt(len(X))))\n",
    "        if verbose:\n",
    "            print(\"Chose n_neighbors automatically as:\", n_neighbors)\n",
    "\n",
    "    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')\n",
    "    knn_clf.fit(X, y)\n",
    "\n",
    "    if model_save_path != \"\":\n",
    "        with open(model_save_path, 'wb') as f:\n",
    "            pickle.dump(knn_clf, f)\n",
    "    return knn_clf\n",
    "\n",
    "def predict(X_img_path, knn_clf = None, model_save_path =\"knn_model.sav\", DIST_THRESH = .5):\n",
    "    \"\"\"\n",
    "    recognizes faces in given image, based on a trained knn classifier\n",
    "    :param X_img_path: path to image to be recognized\n",
    "    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\n",
    "    :param model_save_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\n",
    "    :param DIST_THRESH: (optional) distance threshold in knn classification. the larger it is, the more chance of misclassifying an unknown person to a known one.\n",
    "    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\n",
    "        For faces of unrecognized persons, the name 'N/A' will be passed.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isfile(X_img_path) or splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:\n",
    "        raise Exception(\"invalid image path: {}\".format(X_img_path))\n",
    "\n",
    "    if knn_clf is None and model_save_path == \"knn_model.sav\":\n",
    "        raise Exception(\"must supply knn classifier either thourgh knn_clf or model_save_path\")\n",
    "\n",
    "    if knn_clf is None:\n",
    "        with open(model_save_path, 'rb') as f:\n",
    "            knn_clf = pickle.load(f)\n",
    "\n",
    "    X_img = face_recognition.load_image_file(X_img_path)\n",
    "    X_faces_loc = face_locations(X_img)\n",
    "    if len(X_faces_loc) == 0:\n",
    "        return []\n",
    "\n",
    "    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_faces_loc)\n",
    "\n",
    "\n",
    "    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)\n",
    "\n",
    "    is_recognized = [closest_distances[0][i][0] <= DIST_THRESH for i in range(len(X_faces_loc))]\n",
    "    #print(knn_clf.predict(faces_encodings))\n",
    "    # predict classes and cull classifications that are not with high confidence\n",
    "    return [(pred, loc) if rec else (\"N/A\", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_faces_loc, is_recognized)]\n",
    "\n",
    "def draw_preds(img_path, preds):\n",
    "    \"\"\"\n",
    "    shows the face recognition results visually.\n",
    "    :param img_path: path to image to be recognized\n",
    "    :param preds: results of the predict function\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    source_img = Image.open(img_path).convert(\"RGBA\")\n",
    "    draw = ImageDraw.Draw(source_img)\n",
    "    for pred in preds:\n",
    "        loc = pred[1]\n",
    "        name = pred[0]\n",
    "        # (top, right, bottom, left) => (left,top,right,bottom)\n",
    "        draw.rectangle(((loc[3], loc[0]), (loc[1],loc[2])), outline=\"red\")\n",
    "        draw.text((loc[3], loc[0] - 30), name, font=ImageFont.truetype('arial.ttf', 30),fill=(255,255,255,0))\n",
    "    source_img.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    knn_clf = train(\"knn_examples/train\")\n",
    "    #knn_clf = pickle.load(open(\"knn_model.sav\", 'rb'))\n",
    "    \n",
    "\n",
    "    \n",
    "    for img_path in listdir(\"knn_examples/test\"):\n",
    "        preds = predict(join(\"knn_examples/test\", img_path) ,knn_clf=knn_clf)\n",
    "        print(preds)\n",
    "        draw_preds(join(\"knn_examples/test\", img_path), preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model by using webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_3977de4c_6735_11e8_8a81_b8763fe32a3c tr:hover {\n",
       "          background-color: #ffff99;\n",
       "    }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3c th {\n",
       "          font-size: 150%;\n",
       "          text-align: center;\n",
       "    }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3c caption {\n",
       "          caption-side: center;\n",
       "    }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow0_col0 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow0_col1 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow0_col2 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow0_col3 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow1_col0 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow1_col1 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow1_col2 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow1_col3 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow2_col0 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow2_col1 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow2_col2 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow2_col3 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow3_col0 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow3_col1 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow3_col2 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow3_col3 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow4_col0 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow4_col1 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow4_col2 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow4_col3 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow5_col0 {\n",
       "            font-size:  11pt;\n",
       "            background-color:  greenyellow;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow5_col1 {\n",
       "            font-size:  11pt;\n",
       "            background-color:  greenyellow;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow5_col2 {\n",
       "            font-size:  11pt;\n",
       "            background-color:  greenyellow;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow5_col3 {\n",
       "            font-size:  11pt;\n",
       "            background-color:  greenyellow;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow6_col0 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow6_col1 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow6_col2 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow6_col3 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow7_col0 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow7_col1 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow7_col2 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow7_col3 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow8_col0 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow8_col1 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow8_col2 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow8_col3 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow9_col0 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow9_col1 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow9_col2 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }    #T_3977de4c_6735_11e8_8a81_b8763fe32a3crow9_col3 {\n",
       "            font-size:  11pt;\n",
       "            : ;\n",
       "        }</style>  \n",
       "<table id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3c\" ><caption>Attendence Sheet.</caption> \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >Roll No.</th> \n",
       "        <th class=\"col_heading level0 col1\" >Name</th> \n",
       "        <th class=\"col_heading level0 col2\" >P/A</th> \n",
       "        <th class=\"col_heading level0 col3\" >Time</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3clevel0_row0\" class=\"row_heading level0 row0\" >0</th> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow0_col0\" class=\"data row0 col0\" >101</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow0_col1\" class=\"data row0 col1\" >Aditya</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow0_col2\" class=\"data row0 col2\" >-</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow0_col3\" class=\"data row0 col3\" >-</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3clevel0_row1\" class=\"row_heading level0 row1\" >1</th> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow1_col0\" class=\"data row1 col0\" >102</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow1_col1\" class=\"data row1 col1\" >Aefaaz</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow1_col2\" class=\"data row1 col2\" >-</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow1_col3\" class=\"data row1 col3\" >-</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3clevel0_row2\" class=\"row_heading level0 row2\" >2</th> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow2_col0\" class=\"data row2 col0\" >103</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow2_col1\" class=\"data row2 col1\" >Amogh</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow2_col2\" class=\"data row2 col2\" >-</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow2_col3\" class=\"data row2 col3\" >-</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3clevel0_row3\" class=\"row_heading level0 row3\" >3</th> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow3_col0\" class=\"data row3 col0\" >104</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow3_col1\" class=\"data row3 col1\" >Ashutosh</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow3_col2\" class=\"data row3 col2\" >-</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow3_col3\" class=\"data row3 col3\" >-</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3clevel0_row4\" class=\"row_heading level0 row4\" >4</th> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow4_col0\" class=\"data row4 col0\" >105</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow4_col1\" class=\"data row4 col1\" >Harsh</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow4_col2\" class=\"data row4 col2\" >-</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow4_col3\" class=\"data row4 col3\" >-</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3clevel0_row5\" class=\"row_heading level0 row5\" >5</th> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow5_col0\" class=\"data row5 col0\" >106</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow5_col1\" class=\"data row5 col1\" >Karan</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow5_col2\" class=\"data row5 col2\" >P</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow5_col3\" class=\"data row5 col3\" >19:21:31</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3clevel0_row6\" class=\"row_heading level0 row6\" >6</th> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow6_col0\" class=\"data row6 col0\" >107</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow6_col1\" class=\"data row6 col1\" >Nitish</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow6_col2\" class=\"data row6 col2\" >-</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow6_col3\" class=\"data row6 col3\" >-</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3clevel0_row7\" class=\"row_heading level0 row7\" >7</th> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow7_col0\" class=\"data row7 col0\" >108</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow7_col1\" class=\"data row7 col1\" >Sarosh</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow7_col2\" class=\"data row7 col2\" >-</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow7_col3\" class=\"data row7 col3\" >-</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3clevel0_row8\" class=\"row_heading level0 row8\" >8</th> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow8_col0\" class=\"data row8 col0\" >109</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow8_col1\" class=\"data row8 col1\" >Shivam</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow8_col2\" class=\"data row8 col2\" >-</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow8_col3\" class=\"data row8 col3\" >-</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3clevel0_row9\" class=\"row_heading level0 row9\" >9</th> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow9_col0\" class=\"data row9 col0\" >110</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow9_col1\" class=\"data row9 col1\" >Shreyas</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow9_col2\" class=\"data row9 col2\" >-</td> \n",
       "        <td id=\"T_3977de4c_6735_11e8_8a81_b8763fe32a3crow9_col3\" class=\"data row9 col3\" >-</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x45d5164eb8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import pickle\n",
    "from sklearn import neighbors\n",
    "import time\n",
    "\n",
    "# This is a demo of running face recognition on live video from your webcam. It's a little more complicated than the\n",
    "# other example, but it includes some basic performance tweaks to make things run a lot faster:\n",
    "#   1. Process each video frame at 1/4 resolution (though still display it at full resolution)\n",
    "#   2. Only detect faces in every other frame of video.\n",
    "\n",
    "# PLEASE NOTE: This example requires OpenCV (the `cv2` library) to be installed only to read from your webcam.\n",
    "# OpenCV is *not* required to use the face_recognition library. It's only required if you want to run this\n",
    "# specific demo. If you have trouble installing it, try any of the other demos that don't require it instead.\n",
    "\n",
    "# Get a reference to webcam #0 (the default one)    \n",
    "    \n",
    "\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "present=[]\n",
    "c=[]\n",
    "process_this_frame = True\n",
    "if __name__ == \"__main__\":\n",
    "    #knn_clf = train(\"knn_examples/train\")\n",
    "    knn_clf = pickle.load(open(\"knn_model.sav\", 'rb'))\n",
    "    \n",
    "    while True:\n",
    "        # Grab a single frame of video\n",
    "        ret, frame = video_capture.read()\n",
    "\n",
    "        # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "        rgb_small_frame = small_frame[:, :, ::-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "       \n",
    "        \n",
    "\n",
    "        # Only process every other frame of video to save time\n",
    "        if process_this_frame:\n",
    "            # Find all the faces and face encodings in the current frame of video\n",
    "            face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "            face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "            if (len(face_encodings)>0):\n",
    "                \n",
    "                closest_distances = knn_clf.kneighbors(face_encodings, n_neighbors=1)\n",
    "\n",
    "\n",
    "                is_recognized = [closest_distances[0][i][0] <= 0.5 for i in range(len(face_locations))]\n",
    "\n",
    "                face_names = []\n",
    "                for pred, loc, rec in zip(knn_clf.predict(face_encodings),face_locations, is_recognized):\n",
    "                     if rec:\n",
    "                         face_names.append(pred)\n",
    "                         if pred not in present: \n",
    "                                present.append(pred)\n",
    "                                c.append(time.strftime('%H:%M:%S', time.localtime()))\n",
    "                         \n",
    "                         \n",
    "                        \n",
    "                     else :\n",
    "                         face_names.append(\"unknown\")\n",
    "                 \n",
    "        \n",
    "\n",
    "        process_this_frame = not process_this_frame\n",
    "\n",
    "\n",
    "        # Display the results\n",
    "        for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "            # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "            top *= 4\n",
    "            right *= 4\n",
    "            bottom *= 4\n",
    "            left *= 4\n",
    "\n",
    "            # Draw a box around the face\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "            # Draw a label with a name below the face\n",
    "            cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            #print(name)\n",
    "            cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        # Display the resulting image\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        # Hit 'q' on the keyboard to quit!\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "t=[0,0,0,0,0,0,0,0,0,0]\n",
    "R=['101','102','103','104','105','106','107','108','109','110']\n",
    "n=[ 'Aditya','Aefaaz','Amogh','Ashutosh','Harsh','Karan', 'Nitish', 'Sarosh','Shivam','Shreyas' ]\n",
    "a=[0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(len(present)):\n",
    "    if present[i] in n: \n",
    "        a.pop(n.index(present[i]))\n",
    "        a.insert(n.index(present[i]),'P')\n",
    "        t.pop(n.index(present[i]))\n",
    "        t.insert(n.index(present[i]),c[i])\n",
    "for _ in range(len(a)):\n",
    "        if a[_]==0:\n",
    "            a.pop(_)\n",
    "            a.insert(_,\"-\")\n",
    "            t.pop(_)\n",
    "            t.insert(_,\"-\")\n",
    "data = {'Roll No.':R,'Name':n,'P/A':a,'Time':t}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df=df[['Roll No.','Name','P/A','Time']]\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "def hover(hover_color=\"#ffff99\"):\n",
    "    return dict(selector=\"tr:hover\",\n",
    "                props=[(\"background-color\", \"%s\" % hover_color)])\n",
    "\n",
    "styles = [\n",
    "    hover(),\n",
    "    dict(selector=\"th\", props=[(\"font-size\", \"150%\"),\n",
    "                               (\"text-align\", \"center\")]),\n",
    "    dict(selector=\"caption\", props=[(\"caption-side\", \"center\")])\n",
    "]\n",
    "def highlight(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    z=[]\n",
    "    for i in range(len(df.index)):\n",
    "         if df.iloc[i,2]=='P':\n",
    "                z.append(i)\n",
    "         else:\n",
    "                z.append(-1)     \n",
    "    \n",
    "    return ['background-color: greenyellow' if i==z[i] else ''for i in range(len(z))]\n",
    "html = (df.style.set_table_styles(styles)\n",
    "          .set_caption(\"Attendence Sheet.\").set_properties(**{'font-size':'11pt'})\n",
    "           .apply(highlight))\n",
    "\n",
    "html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
